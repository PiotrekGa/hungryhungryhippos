{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from einops import rearrange, repeat\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    vocab_size = tokenizer.vocab_size + 1\n",
    "    ignored_index = tokenizer.vocab_size\n",
    "    block_size = 20\n",
    "    emb_dim = 16\n",
    "    d_model = 16\n",
    "    m_model = 16\n",
    "    n_heads = 4\n",
    "    n_ssm_heads = 2\n",
    "    n_layer = 2\n",
    "    lr = 0.01\n",
    "    n_epochs = 5\n",
    "    batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, split, block_size, vocab_size, transform=None, target_transform=None) -> None:\n",
    "        super().__init__()\n",
    "        self.split = split\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        dataset = load_dataset(\"glue\", \"mrpc\", split=split)\n",
    "        dataset = [tokenizer(i['sentence1'])['input_ids'] for i in dataset]\n",
    "        self.dataset = [self.pad(x) for x in dataset]\n",
    "        del dataset\n",
    "    \n",
    "    def pad(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.long)\n",
    "        if len(x) == self.block_size + 1:\n",
    "            return x\n",
    "        elif len(x) > self.block_size + 1:\n",
    "            idx = torch.randint(len(x) - self.block_size - 1, (1,))\n",
    "            return x[idx: idx + self.block_size + 1]\n",
    "        else:\n",
    "            n_to_pad = self.block_size + 1 - len(x)\n",
    "            x = F.pad(x, (0, n_to_pad), 'constant', self.vocab_size - 1)\n",
    "            return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx][:-1], self.dataset[idx][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/Users/piotrgabrys/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Found cached dataset glue (/Users/piotrgabrys/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "train_ds = TextDataset('train', CONFIG.block_size, CONFIG.vocab_size)\n",
    "valid_ds = TextDataset('validation', CONFIG.block_size, CONFIG.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, CONFIG.batch_size, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_ds, CONFIG.batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.rand(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.        , 0.57100506, 0.03805074]],\n",
       "\n",
       "       [[1.        , 0.37132413, 0.8258158 ]],\n",
       "\n",
       "       [[1.        , 0.86566519, 0.20012444]]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[:, None] ** np.arange(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssm = SSM(3, 4, 'diag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 14])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssm.kernel(7).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = torch.exp(ssm.log_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA = (1 + dt * ssm.A / 2) / (1 - dt * ssm.A /2)\n",
    "dB = (dt * ssm.B) / (1 - dt * ssm.A / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000+0.0000j, 0.9966+0.0198j]],\n",
       "\n",
       "        [[1.0000+0.0000j, 0.9828+0.0845j]],\n",
       "\n",
       "        [[1.0000+0.0000j, 0.9624+0.1556j]]], grad_fn=<PowBackward1>)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dA[:, None] ** torch.arange(ssm.A.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0005-0.0021j,  0.0033+0.0028j],\n",
       "        [-0.0864-0.0307j,  0.0102+0.0112j],\n",
       "        [ 0.0073-0.0975j,  0.0345+0.0832j]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dB * ssm.C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSM(nn.Module):\n",
    "    def __init__(self, d_model, m_model, mode, s4dlin=True, use_fft=True) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.m_model = m_model\n",
    "        self.mode = mode\n",
    "        self.s4dlin = s4dlin\n",
    "        self.use_fft = use_fft\n",
    "        self.df_min = None\n",
    "\n",
    "        if self.s4dlin:\n",
    "            A = - 0.5 + 1j * torch.pi * torch.arange(self.m_model // 2)\n",
    "            if self.mode == 'diag':\n",
    "                A = repeat(A, 'h -> n h', n=self.d_model)\n",
    "            elif self.mode == 'shift':\n",
    "                A = torch.cat([A[:, :-1], torch.zeros(self.d_model, 1)], dim=1)\n",
    "            else:\n",
    "                raise ValueError('not such mode')\n",
    "            B = torch.ones((self.d_model, self.m_model // 2)) + 0j\n",
    "            C = torch.randn(self.d_model, self.m_model // 2) + 1j * torch.randn(self.d_model, self.m_model // 2)\n",
    "            dt_min = 1e-3\n",
    "            dt_max = 1e-1\n",
    "            self.log_dt = torch.rand(self.d_model) * (math.log(dt_max) - math.log(dt_min)) + torch.log(torch.tensor([dt_min]))\n",
    "            self.log_dt = self.log_dt.reshape(-1, 1)\n",
    "        else:\n",
    "            if self.mode == 'diag':\n",
    "                A = torch.randn((self.d_model, self.m_model)) / 100\n",
    "            elif self.mode == 'shift':\n",
    "                A = torch.cat([torch.randn((self.d_model, self.m_model - 1)), torch.zeros((self.d_model, 1))], dim=1)\n",
    "            else:\n",
    "                raise ValueError('not such mode')\n",
    "            B = torch.randn((self.d_model, self.m_model)) / 100\n",
    "            C = torch.randn((self.d_model, self.m_model)) / 100\n",
    "\n",
    "        self.A = nn.Parameter(A)\n",
    "        self.B = nn.Parameter(B)\n",
    "        self.C = nn.Parameter(C)\n",
    "        D = torch.randn((self.d_model)) / 100\n",
    "        self.D = nn.Parameter(D)\n",
    "\n",
    "    def kernel(self, L):\n",
    "        dt = torch.exp(self.log_dt)\n",
    "        dA = (1 + dt * self.A / 2) / (1 - dt * self.A /2)\n",
    "        dB = (dt * self.B) / (1 - dt * self.A / 2)\n",
    "        return torch.cat([((dB * self.C) * (dA[:, None] ** torch.arange(self.A.shape[1])) ** i).sum(1).real for i in range(L)], dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B T C\n",
    "        block_size = x.shape[1]\n",
    "\n",
    "        if self.s4dlin:\n",
    "            filter = self.kernel(block_size)\n",
    "        else:\n",
    "            filter = [(self.C * (self.A ** i) * self.B).sum(1) for i in range(block_size)]\n",
    "            filter = torch.stack(filter, dim=1).squeeze()\n",
    "            filter = rearrange(filter, 'b c -> c b')\n",
    "\n",
    "        if self.use_fft:\n",
    "            conv = torch.fft.ifft(torch.fft.fft(x) * torch.fft.fft(filter))\n",
    "        else:\n",
    "            x = x.flip(1)\n",
    "            conv_part = filter * x\n",
    "            conv = conv_part.cumsum(dim=1)\n",
    "            conv = conv.flip(1)\n",
    "\n",
    "        y = conv + self.D * x\n",
    "\n",
    "        # batch, block, emb_dim\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H3Head(nn.Module):\n",
    "    \"\"\" one head of H3 \"\"\"\n",
    "\n",
    "    def __init__(self, emb_size, head_size, n_ssm_heads, m_model):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.head_size = head_size\n",
    "        self.n_ssm_heads = n_ssm_heads\n",
    "        self.m_model = m_model\n",
    "\n",
    "        self.key = nn.Linear(self.emb_size, head_size, bias=False)\n",
    "        self.ln_key = nn.LayerNorm(head_size)\n",
    "        self.query = nn.Linear(self.emb_size, head_size, bias=False)\n",
    "        self.ln_query = nn.LayerNorm(head_size)\n",
    "        self.value = nn.Linear(self.emb_size, head_size, bias=False)\n",
    "        self.ln_value = nn.LayerNorm(head_size)\n",
    "        self.ssm_shift = SSM(self.head_size, self.m_model, 'shift')\n",
    "\n",
    "        self.values = nn.ModuleList([nn.Linear(self.head_size, int(self.head_size / self.n_ssm_heads), bias=False) for _ in range(self.n_ssm_heads)])\n",
    "        self.ssm_diags = nn.ModuleList([SSM(int(self.head_size / self.n_ssm_heads), self.m_model, 'diag') for _ in range(self.n_ssm_heads)])\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        k = self.key(x)\n",
    "        k = self.ln_key(k)\n",
    "        k = self.ssm_shift(k)\n",
    "        v = self.value(x)\n",
    "        v = self.ln_key(v)\n",
    "        v = v * k\n",
    "        v = torch.cat([ssm_diag(value(v)) for value, ssm_diag in zip(self.values, self.ssm_diags)], dim=2)\n",
    "        q = self.query(x)\n",
    "        q = self.ln_key(q)\n",
    "        q = v * q\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadH3(nn.Module):\n",
    "    \"\"\" multiple heads of H3 in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, emb_size, num_heads, head_size, n_ssm_heads, m_model, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([H3Head(emb_size, head_size, n_ssm_heads, m_model) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H3Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, emb_size, num_heads, n_ssm_heads, m_model):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = emb_size // num_heads\n",
    "        self.sa = MultiHeadH3(emb_size, num_heads, head_size, n_ssm_heads, m_model)\n",
    "        self.ffwd = FeedFoward(emb_size)\n",
    "        self.ln1 = nn.LayerNorm(emb_size)\n",
    "        self.ln2 = nn.LayerNorm(emb_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H3LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, d_model, m_model, n_heads, n_ssm_heads, n_layer) -> None:\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.d_model = d_model\n",
    "        self.m_model = m_model\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.emb = nn.Embedding(self.vocab_size, self.emb_dim)\n",
    "\n",
    "        self.blocks = nn.Sequential(*[H3Block(emb_dim, n_heads, n_ssm_heads, m_model) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(emb_dim) # final layer norm\n",
    "        self.lm_head = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B, T\n",
    "        x = self.emb(x)\n",
    "        \n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        x = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        x = rearrange(x, 'B T vocab -> (B T) vocab')\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = H3LanguageModel(vocab_size=CONFIG.vocab_size, emb_dim=CONFIG.emb_dim, d_model=CONFIG.d_model, m_model=CONFIG.m_model, n_heads=CONFIG.n_heads, n_ssm_heads=CONFIG.n_ssm_heads, n_layer=CONFIG.n_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(ignore_index=CONFIG.ignored_index)\n",
    "optimizer = torch.optim.Adam(model.parameters(), CONFIG.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    model.eval()\n",
    "\n",
    "    train_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        preds = model(x)\n",
    "        train_loss += loss_func(preds, y.ravel())\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    valid_loss = 0\n",
    "    for x, y in valid_loader:\n",
    "        preds = model(x)\n",
    "        valid_loss += loss_func(preds, y.ravel())\n",
    "    valid_loss /= len(valid_loader)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type ComplexFloat but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m      3\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad(set_to_none\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m     preds \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m      5\u001b[0m     loss \u001b[39m=\u001b[39m loss_func(preds, y\u001b[39m.\u001b[39mravel())\n\u001b[1;32m      6\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/magic-Ij777sAq/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[42], line 32\u001b[0m, in \u001b[0;36mH3LanguageModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     29\u001b[0m     \u001b[39m# B, T\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb(x)\n\u001b[0;32m---> 32\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblocks(x) \u001b[39m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_f(x) \u001b[39m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(x) \u001b[39m# (B,T,vocab_size)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/magic-Ij777sAq/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/magic-Ij777sAq/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/magic-Ij777sAq/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[41], line 14\u001b[0m, in \u001b[0;36mH3Block.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 14\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msa(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln1(x))\n\u001b[1;32m     15\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffwd(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln2(x))\n\u001b[1;32m     16\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/magic-Ij777sAq/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[39], line 11\u001b[0m, in \u001b[0;36mMultiHeadH3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 11\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([h(x) \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj(out))\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "Cell \u001b[0;32mIn[39], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 11\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([h(x) \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj(out))\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/magic-Ij777sAq/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[38], line 30\u001b[0m, in \u001b[0;36mH3Head.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_key(v)\n\u001b[1;32m     29\u001b[0m v \u001b[39m=\u001b[39m v \u001b[39m*\u001b[39m k\n\u001b[0;32m---> 30\u001b[0m v \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([ssm_diag(value(v)) \u001b[39mfor\u001b[39;00m value, ssm_diag \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssm_diags)], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     31\u001b[0m q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery(x)\n\u001b[1;32m     32\u001b[0m q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_key(q)\n",
      "Cell \u001b[0;32mIn[38], line 30\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_key(v)\n\u001b[1;32m     29\u001b[0m v \u001b[39m=\u001b[39m v \u001b[39m*\u001b[39m k\n\u001b[0;32m---> 30\u001b[0m v \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([ssm_diag(value(v)) \u001b[39mfor\u001b[39;00m value, ssm_diag \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssm_diags)], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     31\u001b[0m q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery(x)\n\u001b[1;32m     32\u001b[0m q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_key(q)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/magic-Ij777sAq/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/magic-Ij777sAq/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type ComplexFloat but found Float"
     ]
    }
   ],
   "source": [
    "for i in range(CONFIG.n_epochs):\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        preds = model(x)\n",
    "        loss = loss_func(preds, y.ravel())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss, valid_loss = estimate_loss()\n",
    "    print(f'EPOCH {i}, train loss: {train_loss:.6f}, valid loss: {valid_loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magic-Ij777sAq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e455ffde316e0e59f235fb716f046746ab75667a470758045ed86161d928d35d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
